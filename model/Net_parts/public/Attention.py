#!/usr/bin/env python
# -*- encoding: utf-8 -*-
'''
@File    :   Attention.py
@Time    :   2025/07/02 15:00:43
@Author  :   angkangyu 
@Description: å„ç§æ³¨æ„åŠ›æœºåˆ¶çš„å®˜æ–¹æ ‡å‡†å®ç°
'''

import torch
import torch.nn as nn
import torch.nn.functional as F
import math


class CoordinateAttention(nn.Module):
    """
    Coordinate Attention for Efficient Mobile Network Design
    å®˜æ–¹è®ºæ–‡: https://arxiv.org/abs/2103.02907
    ğŸ”¥ ä¿®å¤ç‰ˆæœ¬ï¼šç¡®ä¿è¾“å‡ºé€šé“æ•°ä¸è¾“å…¥é€šé“æ•°åŒ¹é…
    """
    def __init__(self, inp, reduction=32):  # ğŸ”¥ ä¿®æ”¹ï¼šç§»é™¤oupå‚æ•°ï¼Œè¾“å‡ºé€šé“æ•°ç­‰äºè¾“å…¥é€šé“æ•°
        super(CoordinateAttention, self).__init__()
        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))     # å¯¹è¾“å…¥ç‰¹å¾å›¾åœ¨é«˜åº¦æ–¹å‘è¿›è¡Œå¹³å‡æ± åŒ–
        self.pool_w = nn.AdaptiveAvgPool2d((1, None))     # å¯¹è¾“å…¥ç‰¹å¾å›¾åœ¨å®½åº¦æ–¹å‘è¿›è¡Œå¹³å‡æ± åŒ–

        mip = max(8, inp // reduction)                    # ä¸­é—´å±‚é€šé“æ•°ï¼Œè‡³å°‘ä¸º8ï¼Œé€šå¸¸ä¸ºè¾“å…¥é€šé“æ•°çš„1/32  

        self.conv1 = nn.Conv2d(inp, mip, kernel_size=1, stride=1, padding=0)  # 1x1å·ç§¯ï¼Œ inpé€šé“æ•°åˆ°mipé€šé“æ•°
        self.bn1 = nn.BatchNorm2d(mip)          # æ‰¹å½’ä¸€åŒ–ï¼Œ ç”¨äºç¨³å®šè®­ç»ƒè¿‡ç¨‹
        self.act = nn.ReLU()                # æ¿€æ´»å‡½æ•°ï¼Œé€šå¸¸ä½¿ç”¨ReLU

        # ğŸ”¥ ä¿®å¤ï¼šè¾“å‡ºé€šé“æ•°åº”è¯¥ä¸è¾“å…¥é€šé“æ•°ç›¸åŒ
        self.conv_h = nn.Conv2d(mip, inp, kernel_size=1, stride=1, padding=0)   # å¯¹é«˜åº¦æ–¹å‘ç‰¹å¾å›¾è¿›è¡Œ1x1å·ç§¯ï¼Œ mipé€šé“æ•°åˆ°è¾“å…¥é€šé“æ•°
        self.conv_w = nn.Conv2d(mip, inp, kernel_size=1, stride=1, padding=0)   # å¯¹å®½åº¦æ–¹å‘ç‰¹å¾å›¾è¿›è¡Œ1x1å·ç§¯ï¼Œ mipé€šé“æ•°åˆ°è¾“å…¥é€šé“æ•°

    def forward(self, x):
        identity = x                 # ä¿ç•™è¾“å…¥ç‰¹å¾å›¾                    
        n, c, h, w = x.size()       # è·å–è¾“å…¥ç‰¹å¾å›¾çš„å°ºå¯¸
        
        x_h = self.pool_h(x)        # å¯¹è¾“å…¥ç‰¹å¾å›¾åœ¨é«˜åº¦æ–¹å‘è¿›è¡Œå¹³å‡æ± åŒ– [B, C, H, 1]
        x_w = self.pool_w(x).permute(0, 1, 3, 2)        # å¯¹è¾“å…¥ç‰¹å¾å›¾åœ¨å®½åº¦æ–¹å‘è¿›è¡Œå¹³å‡æ± åŒ–ï¼Œå¹¶è°ƒæ•´ç»´åº¦é¡ºåº [B, C, W, 1]

        y = torch.cat([x_h, x_w], dim=2)  # åœ¨ç©ºé—´ç»´åº¦ä¸Šæ‹¼æ¥ä¸¤ä¸ªæ± åŒ–ç»“æœï¼Œå¾—åˆ°ä¸€ä¸ªæ–°çš„ç‰¹å¾å›¾ï¼Œå°ºå¯¸ä¸º [B, C, H+W, 1]
        
        y = self.conv1(y)       # 1x1å·ç§¯é™ç»´ [B, mip, H+W, 1]
        y = self.bn1(y)         # æ‰¹å½’ä¸€åŒ–
        y = self.act(y)         # æ¿€æ´»å‡½æ•°

        x_h, x_w = torch.split(y, [h, w], dim=2)    # å°†æ‹¼æ¥åçš„ç‰¹å¾å›¾åˆ†å‰²å›é«˜åº¦å’Œå®½åº¦æ–¹å‘çš„ç‰¹å¾å›¾
        x_w = x_w.permute(0, 1, 3, 2)               # è°ƒæ•´å®½åº¦ç‰¹å¾å›¾çš„ç»´åº¦é¡ºåº 

        a_h = self.conv_h(x_h).sigmoid()            # å¯¹é«˜åº¦ç‰¹å¾å›¾è¿›è¡Œ1x1å·ç§¯å¹¶åº”ç”¨sigmoidæ¿€æ´» [B, C, H, 1]
        a_w = self.conv_w(x_w).sigmoid()            # å¯¹å®½åº¦ç‰¹å¾å›¾è¿›è¡Œ1x1å·ç§¯å¹¶åº”ç”¨sigmoidæ¿€æ´» [B, C, 1, W]

        # ğŸ”¥ ä¿®å¤ï¼šç°åœ¨æ‰€æœ‰å¼ é‡éƒ½æ˜¯[B, C, H, W]çš„å½¢çŠ¶ï¼Œå¯ä»¥æ­£ç¡®ç›¸ä¹˜
        out = identity * a_h * a_w                  # å°†è¾“å…¥ç‰¹å¾å›¾ä¸é«˜åº¦å’Œå®½åº¦æ–¹å‘çš„æ³¨æ„åŠ›æƒé‡ç›¸ä¹˜

        return out


class SEAttention(nn.Module):
    """
    Squeeze-and-Excitation Networks
    å®˜æ–¹è®ºæ–‡: https://arxiv.org/abs/1709.01507
    """
    def __init__(self, channel, reduction=16):
        super(SEAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channel, channel // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channel // reduction, channel, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y.expand_as(x)


class ChannelAttention(nn.Module):
    """
    CBAMçš„Channel Attentionéƒ¨åˆ†
    å®˜æ–¹è®ºæ–‡: https://arxiv.org/abs/1807.06521
    """
    def __init__(self, in_planes, ratio=16):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
           
        self.fc = nn.Sequential(nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),
                               nn.ReLU(),
                               nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False))
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.fc(self.avg_pool(x))
        max_out = self.fc(self.max_pool(x))
        out = avg_out + max_out
        return self.sigmoid(out)


class SpatialAttention(nn.Module):
    """
    CBAMçš„Spatial Attentionéƒ¨åˆ†
    å®˜æ–¹è®ºæ–‡: https://arxiv.org/abs/1807.06521
    """
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()

        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x = torch.cat([avg_out, max_out], dim=1)
        x = self.conv1(x)
        return self.sigmoid(x)


class CBAM(nn.Module):
    """
    Convolutional Block Attention Module
    å®˜æ–¹è®ºæ–‡: https://arxiv.org/abs/1807.06521
    å®˜æ–¹ä»£ç : https://github.com/Jongchan/attention-module
    """
    def __init__(self, in_planes, ratio=16, kernel_size=7):
        super(CBAM, self).__init__()
        self.ca = ChannelAttention(in_planes, ratio)
        self.sa = SpatialAttention(kernel_size)

    def forward(self, x):
        out = x * self.ca(x)
        result = out * self.sa(out)
        return result


class ECAAttention(nn.Module):
    """
    Efficient Channel Attention for Deep Convolutional Neural Networks
    å®˜æ–¹è®ºæ–‡: https://arxiv.org/abs/1910.03151
    å®˜æ–¹ä»£ç : https://github.com/BangguWu/ECANet
    """
    def __init__(self, channel, gamma=2, b=1):
        super(ECAAttention, self).__init__()
        t = int(abs((math.log(channel, 2) + b) / gamma))
        k = t if t % 2 else t + 1
        
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.conv = nn.Conv1d(1, 1, kernel_size=k, padding=(k - 1) // 2, bias=False) 
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        b, c, h, w = x.size()
        
        # Global average pooling
        y = self.avg_pool(x).view(b, 1, c)
        
        # Two different branches of ECA module
        y = self.conv(y)
        
        # Multi-scale information fusion
        y = self.sigmoid(y).view(b, c, 1, 1)
        
        return x * y.expand_as(x)


class SimAM(nn.Module):
    """
    SimAM: A Simple, Parameter-Free Attention Module for Convolutional Neural Networks
    å®˜æ–¹è®ºæ–‡: https://proceedings.mlr.press/v139/yang21o.html
    å®˜æ–¹ä»£ç : https://github.com/ZjjConan/SimAM
    """
    def __init__(self, e_lambda=1e-4):
        super(SimAM, self).__init__()
        self.activaton = nn.Sigmoid()
        self.e_lambda = e_lambda

    def __repr__(self):
        s = self.__class__.__name__ + '('
        s += ('lambda=%f)' % self.e_lambda)
        return s

    @staticmethod
    def get_module_name():
        return "simam"

    def forward(self, x):
        b, c, h, w = x.size()
        
        n = w * h - 1
        
        x_minus_mu_square = (x - x.mean(dim=[2,3], keepdim=True)).pow(2)
        y = x_minus_mu_square / (4 * (x_minus_mu_square.sum(dim=[2,3], keepdim=True) / n + self.e_lambda)) + 0.5

        return x * self.activaton(y)


class SerialAttention(nn.Module):
    """
    ä¸²è”æ³¨æ„åŠ›æœºåˆ¶ï¼š(a)CBAM â†’ Re-weight â†’ (b)CoordinateAttention â†’ Re-weight â†’ (c)SE â†’ Re-weight â†’ Output
    """
    def __init__(self, channels, reduction_cbam=16, reduction_se=16, reduction_coord=32, kernel_size_cbam=7):
        super(SerialAttention, self).__init__()
        # (a) CBAM
        self.cbam = CBAM(channels, ratio=reduction_cbam, kernel_size=kernel_size_cbam)
        # (b) Coordinate Attention
        self.coord = CoordinateAttention(channels, channels, reduction=reduction_coord)
        # (c) SE
        self.se = SEAttention(channels, reduction=reduction_se)

    def forward(self, x):
        # (a) CBAM
        x1 = x * self.cbam(x)         # Re-weight after CBAM
        # (b) Coordinate Attention
        x2= self.coord(x1)        # Coordinate Attention
        # (c) SE
        x3 = x1 * self.se(x2)           # Re-weight after SE
        return x3
    
    
    


class SelfAttention(nn.Module):
    """
    è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¨¡å—
    """
    def __init__(self, in_dim, reduction=8):
        super(SelfAttention, self).__init__()
        self.chanel_in = in_dim
        self.reduction = reduction
        
        # ç”ŸæˆQuery, Key, Valueçš„å·ç§¯å±‚
        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//reduction, kernel_size=1)
        self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//reduction, kernel_size=1)
        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)
        self.gamma = nn.Parameter(torch.zeros(1))
        
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x):
        """
        inputs:
            x : input feature maps (B X C X H X W)
        returns:
            out : self attention value + input feature
            attention: B X N X N (N is Width*Height)
        """
        m_batchsize, C, height, width = x.size()
        
        # ç”ŸæˆQuery, Key, Value
        proj_query = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0, 2, 1)  # B X (H*W) X C//reduction
        proj_key = self.key_conv(x).view(m_batchsize, -1, width*height)  # B X C//reduction X (H*W)
        proj_value = self.value_conv(x).view(m_batchsize, -1, width*height)  # B X C X (H*W)

        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        energy = torch.bmm(proj_query, proj_key)  # B X (H*W) X (H*W)
        attention = self.softmax(energy)  # B X (H*W) X (H*W)
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        out = torch.bmm(proj_value, attention.permute(0, 2, 1))  # B X C X (H*W)
        out = out.view(m_batchsize, C, height, width)  # B X C X H X W
        
        # æ®‹å·®è¿æ¥
        out = self.gamma * out + x
        return out


class SelfSEAttention(nn.Module):
    """
    è‡ªæ³¨æ„åŠ›æœºåˆ¶å’ŒSEæ³¨æ„åŠ›æœºåˆ¶ç»“åˆçš„å¤åˆæ³¨æ„åŠ›æ¨¡å—
    å…ˆé€šè¿‡è‡ªæ³¨æ„åŠ›å»ºæ¨¡ç©ºé—´é•¿ç¨‹ä¾èµ–ï¼Œå†é€šè¿‡SEæ³¨æ„åŠ›è¿›è¡Œé€šé“é‡è¦æ€§åŠ æƒ
    """
    def __init__(self, channels, self_reduction=8, se_reduction=16):
        super(SelfSEAttention, self).__init__()
        # è‡ªæ³¨æ„åŠ›æ¨¡å—
        self.self_attention = SelfAttention(channels, reduction=self_reduction)
        # SEæ³¨æ„åŠ›æ¨¡å—
        self.se_attention = SEAttention(channels, reduction=se_reduction)
        
        # å¯é€‰çš„èåˆæƒé‡å‚æ•°
        self.alpha = nn.Parameter(torch.ones(1))
        self.beta = nn.Parameter(torch.ones(1))

    def forward(self, x):
        # å…ˆåº”ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶
        self_out = self.self_attention(x)
        
        # å†åº”ç”¨SEæ³¨æ„åŠ›æœºåˆ¶
        se_out = self.se_attention(self_out)
        
        # åŠ æƒèåˆåŸå§‹è¾“å…¥å’Œä¸¤ç§æ³¨æ„åŠ›çš„è¾“å‡º
        out = self.alpha * self_out + self.beta * se_out
        
        return out


class ParallelSelfSEAttention(nn.Module):
    """
    å¹¶è¡Œç‰ˆæœ¬çš„è‡ªæ³¨æ„åŠ›å’ŒSEæ³¨æ„åŠ›ç»“åˆæ¨¡å—
    åŒæ—¶è®¡ç®—è‡ªæ³¨æ„åŠ›å’ŒSEæ³¨æ„åŠ›ï¼Œç„¶åè¿›è¡Œç‰¹å¾èåˆ
    """
    def __init__(self, channels, self_reduction=8, se_reduction=16, fusion_mode='add'):
        super(ParallelSelfSEAttention, self).__init__()
        # è‡ªæ³¨æ„åŠ›æ¨¡å—
        self.self_attention = SelfAttention(channels, reduction=self_reduction)
        # SEæ³¨æ„åŠ›æ¨¡å—  
        self.se_attention = SEAttention(channels, reduction=se_reduction)
        
        self.fusion_mode = fusion_mode
        
        if fusion_mode == 'concat':
            # å¦‚æœä½¿ç”¨æ‹¼æ¥èåˆï¼Œéœ€è¦é¢å¤–çš„1x1å·ç§¯é™ç»´
            self.fusion_conv = nn.Conv2d(channels * 2, channels, kernel_size=1)
        elif fusion_mode == 'weighted':
            # å¯å­¦ä¹ çš„èåˆæƒé‡
            self.self_weight = nn.Parameter(torch.ones(1))
            self.se_weight = nn.Parameter(torch.ones(1))

    def forward(self, x):
        # å¹¶è¡Œè®¡ç®—è‡ªæ³¨æ„åŠ›å’ŒSEæ³¨æ„åŠ›
        self_out = self.self_attention(x)
        se_out = self.se_attention(x)
        
        if self.fusion_mode == 'add':
            # ç›´æ¥ç›¸åŠ èåˆ
            out = self_out + se_out
        elif self.fusion_mode == 'multiply':
            # é€å…ƒç´ ç›¸ä¹˜èåˆ
            out = self_out * se_out
        elif self.fusion_mode == 'concat':
            # æ‹¼æ¥åé€šè¿‡1x1å·ç§¯èåˆ
            concat_out = torch.cat([self_out, se_out], dim=1)
            out = self.fusion_conv(concat_out)
        elif self.fusion_mode == 'weighted':
            # å¯å­¦ä¹ æƒé‡èåˆ
            out = self.self_weight * self_out + self.se_weight * se_out
        else:
            raise ValueError(f"Unsupported fusion mode: {self.fusion_mode}")
            
        return out

class CrossFusionAttention(nn.Module):
    """
    è‡ªæ³¨æ„åŠ›å’ŒSEæ³¨æ„åŠ›äº¤å‰èåˆæ¨¡å—
    åœ¨è®¡ç®—è¿‡ç¨‹ä¸­å°±è¿›è¡Œç‰¹å¾äº¤æ¢å’Œèåˆ
    """
    def __init__(self, channels, reduction=8):
        super(CrossFusionAttention, self).__init__()
        self.channels = channels
        self.reduction = reduction
        
        # å…±äº«çš„ç‰¹å¾æå–
        self.query_conv = nn.Conv2d(channels, channels//reduction, 1)
        self.key_conv = nn.Conv2d(channels, channels//reduction, 1)
        self.value_conv = nn.Conv2d(channels, channels, 1)
        
        # SEåˆ†æ”¯çš„å…¨å±€æ± åŒ–å’ŒFCå±‚
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        self.se_fc = nn.Sequential(
            nn.Linear(channels, channels//reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channels//reduction, channels, bias=False),
        )
        
        # äº¤å‰èåˆå±‚
        self.cross_conv = nn.Conv2d(channels * 2, channels, 1)
        self.fusion_conv = nn.Conv2d(channels, channels, 3, padding=1)
        
        self.gamma = nn.Parameter(torch.zeros(1))
        self.beta = nn.Parameter(torch.zeros(1))
        self.softmax = nn.Softmax(dim=-1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        B, C, H, W = x.size()
        
        # 1. è‡ªæ³¨æ„åŠ›åˆ†æ”¯
        query = self.query_conv(x).view(B, -1, H*W).permute(0, 2, 1)  # [B, HW, C//r]
        key = self.key_conv(x).view(B, -1, H*W)                       # [B, C//r, HW] 
        value = self.value_conv(x).view(B, -1, H*W)                   # [B, C, HW]
        
        attention = self.softmax(torch.bmm(query, key))               # [B, HW, HW]
        self_out = torch.bmm(value, attention.permute(0, 2, 1))       # [B, C, HW]
        self_out = self_out.view(B, C, H, W)
        
        # 2. SEåˆ†æ”¯ - ä½†ä½¿ç”¨è‡ªæ³¨æ„åŠ›çš„valueç‰¹å¾
        se_input = value.view(B, C, H, W)  # å…³é”®ï¼šä½¿ç”¨è‡ªæ³¨æ„åŠ›çš„valueç‰¹å¾
        se_weight = self.global_pool(se_input).view(B, C)
        se_weight = self.sigmoid(self.se_fc(se_weight)).view(B, C, 1, 1)
        se_out = se_input * se_weight
        
        # 3. äº¤å‰èåˆ
        cross_feat = torch.cat([self_out, se_out], dim=1)  # [B, 2C, H, W]
        cross_feat = self.cross_conv(cross_feat)           # [B, C, H, W]
        
        # 4. æœ€ç»ˆèåˆ
        fused = self.fusion_conv(cross_feat)
        output = x + self.gamma * self_out + self.beta * fused
        
        return output
    
    
    
class GatedSelfSEAttention(nn.Module):
    """
    ä½¿ç”¨é—¨æ§æœºåˆ¶åŠ¨æ€é€‰æ‹©è‡ªæ³¨æ„åŠ›å’ŒSEæ³¨æ„åŠ›çš„è´¡çŒ®
    """
    def __init__(self, channels, reduction=8):
        super(GatedSelfSEAttention, self).__init__()
        self.channels = channels
        
        # å…±äº«ç‰¹å¾æå–å±‚
        self.shared_conv = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn = nn.BatchNorm2d(channels)
        self.relu = nn.ReLU(inplace=True)
        
        # è‡ªæ³¨æ„åŠ›åˆ†æ”¯
        self.query_conv = nn.Conv2d(channels, channels//reduction, 1)
        self.key_conv = nn.Conv2d(channels, channels//reduction, 1) 
        self.value_conv = nn.Conv2d(channels, channels, 1)
        
        # SEåˆ†æ”¯
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        self.se_fc = nn.Sequential(
            nn.Linear(channels, channels//reduction),
            nn.ReLU(),
            nn.Linear(channels//reduction, channels),
        )
        
        # é—¨æ§ç½‘ç»œ - å†³å®šä¸¤ä¸ªåˆ†æ”¯çš„æƒé‡
        self.gate_conv = nn.Sequential(
            nn.Conv2d(channels, channels//4, 3, padding=1),
            nn.BatchNorm2d(channels//4),
            nn.ReLU(),
            nn.Conv2d(channels//4, 2, 1),  # è¾“å‡º2ä¸ªé€šé“ï¼šself_weight, se_weight
            nn.Softmax(dim=1)
        )
        
        # ç‰¹å¾å¢å¼º
        self.enhance_conv = nn.Sequential(
            nn.Conv2d(channels, channels, 3, padding=1),
            nn.BatchNorm2d(channels),
            nn.ReLU()
        )
        
        self.gamma = nn.Parameter(torch.ones(1))
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x):
        B, C, H, W = x.size()
        
        # å…±äº«ç‰¹å¾æå–
        shared_feat = self.relu(self.bn(self.shared_conv(x)))
        
        # è‡ªæ³¨æ„åŠ›è®¡ç®—
        query = self.query_conv(shared_feat).view(B, -1, H*W).permute(0, 2, 1)
        key = self.key_conv(shared_feat).view(B, -1, H*W)
        value = self.value_conv(shared_feat).view(B, -1, H*W)
        
        attention = self.softmax(torch.bmm(query, key))
        self_out = torch.bmm(value, attention.permute(0, 2, 1)).view(B, C, H, W)
        
        # SEæ³¨æ„åŠ›è®¡ç®—
        se_weight = self.global_pool(shared_feat).view(B, C)
        se_weight = torch.sigmoid(self.se_fc(se_weight)).view(B, C, 1, 1)
        se_out = shared_feat * se_weight
        
        # é—¨æ§æƒé‡è®¡ç®—
        gate_weights = self.gate_conv(shared_feat)  # [B, 2, H, W]
        self_gate = gate_weights[:, 0:1, :, :]      # [B, 1, H, W]
        se_gate = gate_weights[:, 1:2, :, :]        # [B, 1, H, W]
        
        # é—¨æ§èåˆ
        gated_out = self_gate * self_out + se_gate * se_out
        
        # ç‰¹å¾å¢å¼º
        enhanced_out = self.enhance_conv(gated_out)
        
        # æ®‹å·®è¿æ¥
        output = x + self.gamma * enhanced_out
        
        return output
    
    
    
class CA2(nn.Module):
    """
    é­”æ”¹ç‰ˆåæ ‡æ³¨æ„åŠ›æœºåˆ¶
    æ”¹è¿›ç‚¹ï¼š
    1. æ¡å½¢å·ç§¯æ›¿ä»£H/Wå…¨å±€å¹³å‡æ± åŒ–
    2. å¤šå°ºåº¦æ¡å½¢å·ç§¯
    3. è½»é‡é€šé“æ³¨æ„åŠ›å¢å¼º
    4. å…ˆCAå†é—¨æ§èåˆ
    """
    def __init__(self, inp, oup, reduction=32, strip_kernels=[3, 5, 7], use_channel_attention=True):
        super(CA2, self).__init__()
        self.inp = inp
        self.oup = oup
        self.strip_kernels = strip_kernels
        self.use_channel_attention = use_channel_attention
        
        # æ”¹è¿›ç‚¹1&2: å¤šå°ºåº¦æ¡å½¢å·ç§¯æ›¿ä»£å…¨å±€å¹³å‡æ± åŒ–
        self.multi_scale_h_convs = nn.ModuleList([
            nn.Conv2d(inp, inp, kernel_size=(1, k), padding=(0, k//2), groups=inp)
            for k in strip_kernels
        ])
        
        self.multi_scale_w_convs = nn.ModuleList([
            nn.Conv2d(inp, inp, kernel_size=(k, 1), padding=(k//2, 0), groups=inp)
            for k in strip_kernels
        ])
        
        # å¤šå°ºåº¦ç‰¹å¾èåˆ
        self.h_fusion_conv = nn.Conv2d(inp * len(strip_kernels), inp, 1)
        self.w_fusion_conv = nn.Conv2d(inp * len(strip_kernels), inp, 1)
        
        # åŸå§‹çš„æ± åŒ–æ“ä½œä½œä¸ºè¡¥å……
        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))
        self.pool_w = nn.AdaptiveAvgPool2d((1, None))
        
        # ä¸­é—´å±‚é€šé“æ•°
        mip = max(8, inp // reduction)
        
        # å…±äº«çš„1x1å·ç§¯å’Œå½’ä¸€åŒ–
        self.conv1 = nn.Conv2d(inp, mip, kernel_size=1, stride=1, padding=0)
        self.bn1 = nn.BatchNorm2d(mip)
        self.act = nn.ReLU()
        
        # æ–¹å‘ç‰¹å¾ç”Ÿæˆ
        self.conv_h = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)
        self.conv_w = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)
        
        # æ”¹è¿›ç‚¹3: è½»é‡é€šé“æ³¨æ„åŠ›
        if use_channel_attention:
            self.channel_attention = nn.Sequential(
                nn.AdaptiveAvgPool2d(1),
                nn.Conv2d(inp, inp // 16, 1, bias=False),
                nn.ReLU(),
                nn.Conv2d(inp // 16, inp, 1, bias=False),
                nn.Sigmoid()
            )
        
        # æ”¹è¿›ç‚¹4: é—¨æ§èåˆæœºåˆ¶
        self.gate_conv = nn.Sequential(
            nn.Conv2d(inp, inp // 4, 3, padding=1),
            nn.BatchNorm2d(inp // 4),
            nn.ReLU(),
            nn.Conv2d(inp // 4, 3, 1),  # 3ä¸ªé—¨æ§æƒé‡ï¼šåŸå§‹ã€CA_hã€CA_w
            nn.Softmax(dim=1)
        )
        
        # ç‰¹å¾å¢å¼ºå±‚
        self.enhance_conv = nn.Sequential(
            nn.Conv2d(inp, inp, 3, padding=1),
            nn.BatchNorm2d(inp),
            nn.ReLU()
        )

    def forward(self, x):
        identity = x
        n, c, h, w = x.size()
        
        # æ”¹è¿›ç‚¹3: å…ˆåº”ç”¨é€šé“æ³¨æ„åŠ›
        if self.use_channel_attention:
            channel_weight = self.channel_attention(x)
            x_enhanced = x * channel_weight
        else:
            x_enhanced = x
        
        # æ”¹è¿›ç‚¹1&2: å¤šå°ºåº¦æ¡å½¢å·ç§¯ç‰¹å¾æå–
        # Hæ–¹å‘å¤šå°ºåº¦ç‰¹å¾
        h_features = []
        for h_conv in self.multi_scale_h_convs:
            h_feat = h_conv(x_enhanced)  # [B, C, H, W]
            h_feat = F.adaptive_avg_pool2d(h_feat, (None, 1))  # [B, C, H, 1]
            h_features.append(h_feat)
        
        # Wæ–¹å‘å¤šå°ºåº¦ç‰¹å¾
        w_features = []
        for w_conv in self.multi_scale_w_convs:
            w_feat = w_conv(x_enhanced)  # [B, C, H, W]
            w_feat = F.adaptive_avg_pool2d(w_feat, (1, None))  # [B, C, 1, W]
            w_features.append(w_feat)
        
        # èåˆå¤šå°ºåº¦ç‰¹å¾
        x_h = torch.cat(h_features, dim=1)  # [B, C*scales, H, 1]
        x_h = self.h_fusion_conv(x_h)       # [B, C, H, 1]
        
        x_w = torch.cat(w_features, dim=1)  # [B, C*scales, 1, W]
        x_w = self.w_fusion_conv(x_w)       # [B, C, 1, W]
        
        # ç»“åˆåŸå§‹æ± åŒ–ç‰¹å¾ï¼ˆä¿æŒå¤šæ ·æ€§ï¼‰
        x_h_pool = self.pool_h(x_enhanced)
        x_w_pool = self.pool_w(x_enhanced)
        
        # ç‰¹å¾èåˆ
        x_h = 0.7 * x_h + 0.3 * x_h_pool
        x_w = 0.7 * x_w + 0.3 * x_w_pool
        
        # è°ƒæ•´ç»´åº¦å¹¶æ‹¼æ¥
        x_w = x_w.permute(0, 1, 3, 2)
        y = torch.cat([x_h, x_w], dim=2)  # [B, C, H+W, 1]
        
        # å…±äº«çš„ç‰¹å¾å˜æ¢
        y = self.conv1(y)
        y = self.bn1(y)
        y = self.act(y)
        
        # åˆ†ç¦»Hå’ŒWæ–¹å‘ç‰¹å¾
        x_h, x_w = torch.split(y, [h, w], dim=2)
        x_w = x_w.permute(0, 1, 3, 2)
        
        # ç”Ÿæˆæ³¨æ„åŠ›æƒé‡
        a_h = self.conv_h(x_h).sigmoid()  # [B, C, H, 1]
        a_w = self.conv_w(x_w).sigmoid()  # [B, C, 1, W]
        
        # æ”¹è¿›ç‚¹4: é—¨æ§èåˆç­–ç•¥
        # è®¡ç®—CAå¢å¼ºåçš„ç‰¹å¾
        ca_enhanced = identity * a_w * a_h
        
        # è®¡ç®—é—¨æ§æƒé‡
        gate_weights = self.gate_conv(identity)  # [B, 3, H, W]
        identity_gate = gate_weights[:, 0:1, :, :]    # åŸå§‹ç‰¹å¾æƒé‡
        ca_h_gate = gate_weights[:, 1:2, :, :]        # CA_Hæƒé‡  
        ca_w_gate = gate_weights[:, 2:3, :, :]        # CA_Wæƒé‡
        
        # é—¨æ§èåˆ
        gated_output = (identity_gate * identity + 
                       ca_h_gate * (identity * a_h) + 
                       ca_w_gate * (identity * a_w))
        
        # æœ€ç»ˆç‰¹å¾å¢å¼º
        enhanced_output = self.enhance_conv(gated_output)
        
        # æ®‹å·®è¿æ¥
        final_output = identity + enhanced_output
        
        return final_output

class CA3(nn.Module):
    """
    è½»é‡çº§ç‰ˆæœ¬çš„å¢å¼ºåæ ‡æ³¨æ„åŠ›
    é’ˆå¯¹ç§»åŠ¨ç«¯å’Œèµ„æºå—é™ç¯å¢ƒä¼˜åŒ–
    """
    def __init__(self, inp, oup, reduction=32, strip_kernel=5):
        super(CA3, self).__init__()
        
        # å•ä¸€å°ºåº¦æ¡å½¢å·ç§¯ï¼ˆå‡å°‘å‚æ•°ï¼‰
        self.h_conv = nn.Conv2d(inp, inp, kernel_size=(1, strip_kernel), 
                               padding=(0, strip_kernel//2), groups=inp)
        self.w_conv = nn.Conv2d(inp, inp, kernel_size=(strip_kernel, 1), 
                               padding=(strip_kernel//2, 0), groups=inp)
        
        # æ± åŒ–è¡¥å……
        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))
        self.pool_w = nn.AdaptiveAvgPool2d((1, None))
        
        # è½»é‡åŒ–MLP
        mip = max(8, inp // reduction)
        self.conv1 = nn.Conv2d(inp, mip, 1, bias=False)
        self.act = nn.ReLU()
        
        self.conv_h = nn.Conv2d(mip, oup, 1, bias=False)
        self.conv_w = nn.Conv2d(mip, oup, 1, bias=False)
        
        # ç®€åŒ–çš„é€šé“æ³¨æ„åŠ›
        self.channel_gate = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(inp, inp, 1, groups=inp, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        identity = x
        n, c, h, w = x.size()
        
        # è½»é‡é€šé“æ³¨æ„åŠ›
        channel_weight = self.channel_gate(x)
        x = x * channel_weight
        
        # æ¡å½¢å·ç§¯ + æ± åŒ–èåˆ
        x_h = 0.6 * self.h_conv(x) + 0.4 * self.pool_h(x)
        x_w = 0.6 * self.w_conv(x) + 0.4 * self.pool_w(x)
        
        # ç»´åº¦è°ƒæ•´å’Œæ‹¼æ¥
        x_h = F.adaptive_avg_pool2d(x_h, (None, 1))
        x_w = F.adaptive_avg_pool2d(x_w, (1, None))
        x_w = x_w.permute(0, 1, 3, 2)
        
        y = torch.cat([x_h, x_w], dim=2)
        
        # ç‰¹å¾å˜æ¢
        y = self.act(self.conv1(y))
        
        # åˆ†ç¦»å¹¶ç”Ÿæˆæƒé‡
        x_h, x_w = torch.split(y, [h, w], dim=2)
        x_w = x_w.permute(0, 1, 3, 2)
        
        a_h = self.conv_h(x_h).sigmoid()
        a_w = self.conv_w(x_w).sigmoid()
        
        # ç›´æ¥åº”ç”¨æƒé‡
        out = identity * a_w * a_h
        
        return out